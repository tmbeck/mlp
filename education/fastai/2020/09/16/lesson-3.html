<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>fastai v4: Lesson 3 | ML Development Blog</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="fastai v4: Lesson 3" />
<meta name="author" content="<a href='https://www.linkedin.com/in/tmbeck'>Tim Beck</a>" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Notes from Lesson 3 of fastai v4" />
<meta property="og:description" content="Notes from Lesson 3 of fastai v4" />
<link rel="canonical" href="https://blog.tbeck.io/education/fastai/2020/09/16/lesson-3.html" />
<meta property="og:url" content="https://blog.tbeck.io/education/fastai/2020/09/16/lesson-3.html" />
<meta property="og:site_name" content="ML Development Blog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-09-16T00:00:00-05:00" />
<script type="application/ld+json">
{"description":"Notes from Lesson 3 of fastai v4","url":"https://blog.tbeck.io/education/fastai/2020/09/16/lesson-3.html","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://blog.tbeck.io/education/fastai/2020/09/16/lesson-3.html"},"author":{"@type":"Person","name":"<a href='https://www.linkedin.com/in/tmbeck'>Tim Beck</a>"},"headline":"fastai v4: Lesson 3","dateModified":"2020-09-16T00:00:00-05:00","datePublished":"2020-09-16T00:00:00-05:00","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://blog.tbeck.io/feed.xml" title="ML Development Blog" /><link rel="shortcut icon" type="image/x-icon" href="/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />

<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/">ML Development Blog</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About Me</a><a class="page-link" href="/links/">Useful Links</a><a class="page-link" href="/projects/">Projects</a><a class="page-link" href="/search/">Search</a><a class="page-link" href="/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">fastai v4: Lesson 3</h1><p class="page-description">Notes from Lesson 3 of fastai v4</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2020-09-16T00:00:00-05:00" itemprop="datePublished">
        Sep 16, 2020
      </time>• 
          <span itemprop="author" itemscope itemtype="http://schema.org/Person">
            <span class="p-author h-card" itemprop="name"><a href='https://www.linkedin.com/in/tmbeck'>Tim Beck</a></span></span>
       • <span class="read-time" title="Estimated read time">
    
    
      10 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/categories/#education">education</a>
        &nbsp;
      
        <a class="category-tags-link" href="/categories/#fastai">fastai</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h1"><a href="#lesson-3">Lesson 3</a>
<ul>
<li class="toc-entry toc-h2"><a href="#links">Links</a></li>
<li class="toc-entry toc-h2"><a href="#notes">Notes</a>
<ul>
<li class="toc-entry toc-h3"><a href="#from-chapter-4">From Chapter 4</a>
<ul>
<li class="toc-entry toc-h4"><a href="#stop-and-think">Stop and Think</a></li>
<li class="toc-entry toc-h4"><a href="#gradients">Gradients</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#questionnaire">Questionnaire</a></li>
</ul>
</li>
</ul><h1 id="lesson-3">
<a class="anchor" href="#lesson-3" aria-hidden="true"><span class="octicon octicon-link"></span></a>Lesson 3</h1>

<p>Notes from fastai lesson 3, which is Chapter 4 in the book (the end of Ch. 2 is a “choose your own adventure” where you can continue on to Ch. 3, Data Ethics, or Ch. 4, Under the Hood: Training a Digit Classifier). The video lessons pick up at Ch. 4. But After reading much of Ch. 3, I find the Ethics to be an intriguing topic, especially for leaders of data science organizations, where failure to consider Data Ethics might put your organization (or at least its reputation) at risk.</p>

<h2 id="links">
<a class="anchor" href="#links" aria-hidden="true"><span class="octicon octicon-link"></span></a>Links</h2>

<ul>
  <li><a href="https://course.fast.ai/videos/?lesson=3">Lesson 3 Video</a></li>
  <li><a href="https://forums.fast.ai/">fastai Forum</a></li>
  <li><a href="http://jalammar.github.io/visual-numpy/">A visual intro to numpy and data representation</a></li>
</ul>

<h2 id="notes">
<a class="anchor" href="#notes" aria-hidden="true"><span class="octicon octicon-link"></span></a>Notes</h2>

<h3 id="from-chapter-4">
<a class="anchor" href="#from-chapter-4" aria-hidden="true"><span class="octicon octicon-link"></span></a>From Chapter 4</h3>

<p>Covered images as arrays or tensors of pixels</p>

<p>Cool trick:</p>

<p>Visualizing greyscale images inside of a pandas df by specifying a background graident:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">im3_t</span> <span class="o">=</span> <span class="n">tensor</span><span class="p">(</span><span class="n">im3</span><span class="p">)</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">im3_t</span><span class="p">[</span><span class="mi">4</span><span class="p">:</span><span class="mi">15</span><span class="p">,</span><span class="mi">4</span><span class="p">:</span><span class="mi">22</span><span class="p">])</span>
<span class="n">df</span><span class="p">.</span><span class="n">style</span><span class="p">.</span><span class="n">set_properties</span><span class="p">(</span><span class="o">**</span><span class="p">{</span><span class="s">'font-size'</span><span class="p">:</span><span class="s">'6pt'</span><span class="p">}).</span><span class="n">background_gradient</span><span class="p">(</span><span class="s">'Greys'</span><span class="p">)</span>
</code></pre></div></div>

<h4 id="stop-and-think">
<a class="anchor" href="#stop-and-think" aria-hidden="true"><span class="octicon octicon-link"></span></a>Stop and Think</h4>

<blockquote>
  <p>…how a computer might be able to recognize these two digits. What kinds of features might it be able to look at? How might it be able to identify these features? How could it combine them?</p>
</blockquote>

<p>This isn’t a new topic for me as it overlaps with my <em>Signals &amp; Systems</em> coursework, as well as DSP.</p>

<ul>
  <li>Most straightforward: sum(A-B) = similarity score, identical images == 0</li>
  <li>The above, but after running through edge detection (first derivative)</li>
  <li>Use a kerneling function step-wise compare the outputs of the kernel function between the two systems</li>
</ul>

<p>Measuring distance in vectors</p>
<ul>
  <li>
    <p>L1 norm / mean absolute difference: abs(A-B).mean()</p>

    <p>Use <code class="language-plaintext highlighter-rouge">F.l1_loss()</code></p>
  </li>
  <li>
    <p>L2 norm / root mean square error (RMSE): sqrt(mean((A-B)^2))</p>

    <p>Use <code class="language-plaintext highlighter-rouge">F.mse_loss()</code></p>
  </li>
</ul>

<p>When computing <code class="language-plaintext highlighter-rouge">mean()</code> with tensors, <code class="language-plaintext highlighter-rouge">mean()</code> can take an optional tuple argument that specifies the range of axes over which to compute the mean.</p>

<p>Much of this chapter covers an introduction to numpy and tensors. Vectors and matrices aren’t new to me, and even slicing n-dimensional arrays is familiar (I have previous experience in perl and C). But I was struggling with this in python, e.g. the difference between <code class="language-plaintext highlighter-rouge">[:,:]</code> and <code class="language-plaintext highlighter-rouge">[:][:]</code>. I found this article by Jay Alammar very helpful: <a href="http://jalammar.github.io/visual-numpy/">A visual intro to numpy and data representation</a> (thank you Jay!).</p>

<p>Stochastic Gradient Descent (SGD)</p>

<p><img src="/images/sgd.png" alt="SGD" title="Stochastic Gradient Descent"></p>

<p>So far our primitive model is little more than a math function with no memory and no feedback. To do better we need to be able to train it and to make changes to a set of weights so as to alter the next outcome of the model.</p>

<p>Weights for each pixel. Could use the computed means as weights, where the flattened mean are weights.</p>

<p>Consider the simple function <code class="language-plaintext highlighter-rouge">def pr_eight(x,w) = (x*w).sum()</code> where <code class="language-plaintext highlighter-rouge">x</code>, <code class="language-plaintext highlighter-rouge">w</code> are vectors for the independent variables and the weights, respectively. How would we make this a machine learning “8” classifier?</p>

<ol>
  <li>Need to init(w) - seeded rand()?</li>
  <li>For each image in our set, compute <code class="language-plaintext highlighter-rouge">x*w</code> e.g. make a prediction.</li>
  <li>Compute the loss of the model (actual vs. expected)</li>
  <li>Compute the gradient: a measure of change for each weight that changes the loss</li>
  <li>Update the weights</li>
  <li>
<code class="language-plaintext highlighter-rouge">goto</code> 2. until some condition (e.g. the model is good)</li>
</ol>

<p>Initialize</p>

<p>Start with random. How would random compare against the <code class="language-plaintext highlighter-rouge">mean()</code> though?</p>

<p>Loss</p>

<p>The convention is that small loss == good, big loss == bad. So we want a function that will measure ourselves as such.</p>

<p>Step</p>

<p>This is a change to the weight that results in a new prediction. Weights can go up or down. Mentions using gradients (calculus). But not clear how to avoid things like local minimums…</p>

<p>Stop</p>

<p>How we know when to stop - e.g. such as when our model begins to get worse at its job.</p>

<h4 id="gradients">
<a class="anchor" href="#gradients" aria-hidden="true"><span class="octicon octicon-link"></span></a>Gradients</h4>

<ul>
  <li>Gradients are a (continuous? since you must take at least the first derivative) function (used the quadratic y=x**2) with a local minimum (since there’s a stopping point where y’==0?). It’s also symmetric around an axis.</li>
  <li>A random value on the line is chosen (some x) as the initial value</li>
  <li>pytorch provides the <code class="language-plaintext highlighter-rouge">backward()</code> function to compute the gradient of a layer (array) of a network. This is the <em>backward pass</em> through the network. The <em>forward pass</em> is where the activation function is calculated.</li>
  <li>This is all handled through object-oriented “magic” in pytorch.</li>
  <li>stepping is done by altering the weights by your learning rate (<code class="language-plaintext highlighter-rouge">lr</code>) * the gradient of your weights. Recall that <code class="language-plaintext highlighter-rouge">1e-5</code> is a common rate.</li>
  <li>Takeaway: the gradient is a search for a local minimum. Step size through this search is the learning rate. This is done by backpropagating the affect of a function to a tensor’s gradient. In this sense, “all the points” in our array are somewhere on this line and we wish to find a tensor such that the sum of the gradients is closest to zero. Think I said that correctly.</li>
</ul>

<p>Application:</p>

<p>Let’s apply the above to our MNIST imageset…</p>

<h2 id="questionnaire">
<a class="anchor" href="#questionnaire" aria-hidden="true"><span class="octicon octicon-link"></span></a>Questionnaire</h2>

<ol>
  <li>
    <p>What letters are often used to signify the independent and dependent variables?</p>

    <p>x for the independent variable and y for the dependent variable, as in math, e.g. <code class="language-plaintext highlighter-rouge">y = f(x)</code>.</p>
  </li>
  <li>
    <p>What’s the difference between the crop, pad, and squish resize approaches? When might you choose one over the others?</p>

    <p>Crop trims image to size, pad adds zeros (black) to size, squish alters the aspect ratio to size. The choice depends on the data and the subject. It may be advantageous for the model to recognize half a bear, such as if trim is used.</p>
  </li>
  <li>
    <p>What is data augmentation? Why is it needed?</p>

    <p>Data augmentation provides a broader view of material to train on. It can be a way to diversify the training set the model is based on.</p>
  </li>
  <li>
    <p>What is the difference between item_tfms and batch_tfms?</p>

    <p><code class="language-plaintext highlighter-rouge">item_tfms</code> operates on each item in a batch while <code class="language-plaintext highlighter-rouge">batch_tfms</code> operates on each batch.</p>
  </li>
  <li>
    <p>What is a confusion matrix?</p>

    <p>A confusion matrix describes the predictions of the model, showing both where it is right vs. where it is wrong. It is shown as a table where x and y are your labels/categories and each cell is the number of images predicted vs. actually labeled.</p>
  </li>
  <li>
    <p>What does export save?</p>

    <p>Export saves the labels, weights, and biases of the model for use on another system, e.g. to deploy to production.</p>
  </li>
  <li>
    <p>What is it called when we use a model for getting predictions, instead of training?</p>

    <p>This is called inference.</p>
  </li>
  <li>
    <p>What are IPython widgets?</p>

    <p>Controls that couple python, javascript, and html into a functional GUI within a jupyter notebook.</p>
  </li>
  <li>
    <p>When might you want to use CPU for deployment? When might GPU be better?</p>

    <p>For inference (e.g. use in production), a CPU is likely sufficient, however for models deployed at scale, a GPU may be faster.</p>
  </li>
  <li>
    <p>What are the downsides of deploying your app to a server, instead of to a client (or edge) device such as a phone or PC?</p>

    <ol>
      <li>It requires online access</li>
      <li>It is limited by the latency and bandwidth available of the underlying network</li>
      <li>It requires centralized resources (e.g. the server it runs on)</li>
    </ol>
  </li>
  <li>
    <p>What are three examples of problems that could occur when rolling out a bear warning system in practice?</p>

    <ol>
      <li>It could be fooled by costumes or may not recognize partial bear images</li>
      <li>It might not know how to recognize all bears</li>
      <li>If online, requires internet connectivity, if offline, it requires updates</li>
    </ol>
  </li>
  <li>
    <p>What is “out-of-domain data”?</p>

    <p>Data shown to a model that is unlike anything it was trained on</p>
  </li>
  <li>
    <p>What is “domain shift”?</p>

    <p>Where the data our model is shown in production changes over time, diverging from what it was trained on. To resolve this, online training or simply retraining are used.</p>
  </li>
  <li>
    <p>What are the three steps in the deployment process? <em>This is question 27 in Chapter 2 of fastbook</em></p>

    <ol>
      <li>Create (train) the model</li>
      <li>Export the model</li>
      <li>Deploy the model for inference</li>
    </ol>
  </li>
  <li>
    <p>How is a grayscale image represented on a computer? How about a color image?</p>

    <p>Grayscale images are represented as an array of 8-bit values. Color images (at least RGB colorspaced) are represented as a tuple/vector of three 8-bit values. For example, the shape of a greyscale image that is 28 pixels by 28 pixels would be (28, 28) and a color image (28, 28, 3).</p>
  </li>
  <li>
    <p>How are the files and folders in the MNIST_SAMPLE dataset structured? Why?</p>

    <p>The data set has been pre-separated into a training set and a validation set. In each dataset, directories for each numeral contain images of that numeral.</p>
  </li>
  <li>
    <p>Explain how the “pixel similarity” approach to classifying digits works.</p>

    <p>The “pixel similarity” approach used an average of all images of a numeral to create a baseline to compare new numeral images against. This can be done, say, by subtracting the sample image from the average and then using the result to compute an error vector, such as by taking the L1 normal (abs() of the difference minus the mean()).</p>
  </li>
  <li>
    <p>What is a list comprehension? Create one now that selects odd numbers from a list and doubles them.</p>

    <p>List comprehension is a feature of the python language that returns a list from a some function. For example</p>

    <p><code class="language-plaintext highlighter-rouge">doubles = [ (x*2) for x in range(1, 100, 2) ]</code></p>

    <p>Returns a list containing the double of each odd number from 1 up to 100.</p>
  </li>
  <li>
    <p>What is a “rank-3 tensor”?</p>

    <p>A rank-3 tensor is a matrix. Recall that a rank-0 tensor is a scalar and a rank-1 tensor is a vector.</p>
  </li>
  <li>
    <p>What is the difference between tensor rank and shape? How do you get the rank from the shape?</p>

    <p>A tensor shape describes the length of a tensor in each of its dimensions. The rank describes the number of dimensions. The rank can be computed as:</p>

    <p><code class="language-plaintext highlighter-rouge">rank = len(t.shape)</code></p>
  </li>
  <li>
    <p>What are RMSE and L1 norm?</p>

    <p>L1 norm is the mean absolute distance: abs(A-B) - mean(A-B)
L2 norm is the root mean square average: (A-B)**2.mean().sqrt()</p>
  </li>
  <li>
    <p>How can you apply a calculation on thousands of numbers at once, many thousands of times faster than a Python loop?</p>

    <p>Using matrix math in numpy or tensors in pytorch.</p>
  </li>
  <li>
    <p>Create a 3×3 tensor or array containing the numbers from 1 to 9. Double it. Select the bottom-right four numbers.</p>

    <div class="language-python highlighter-rouge">
<div class="highlight"><pre class="highlight"><code><span class="p">(</span><span class="n">t</span><span class="p">.</span><span class="n">tensor</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">10</span><span class="p">)).</span><span class="n">reshape</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="o">*</span><span class="mi">2</span><span class="p">)[</span><span class="mi">1</span><span class="p">:</span><span class="mi">3</span><span class="p">,</span><span class="mi">1</span><span class="p">:</span><span class="mi">3</span><span class="p">]</span>

<span class="n">tensor</span><span class="p">([[</span><span class="mi">10</span><span class="p">,</span>  <span class="mi">12</span><span class="p">],</span>
         <span class="mi">16</span><span class="p">,</span> <span class="mi">18</span><span class="p">]])</span>
</code></pre></div>    </div>
  </li>
  <li>
    <p>What is broadcasting?</p>
  </li>
  <li>
    <p>Are metrics generally calculated using the training set, or the validation set? Why?</p>
  </li>
  <li>
    <p>What is SGD?</p>

    <p>SGD is stochastic gradient descent.</p>
  </li>
  <li>
    <p>Why does SGD use mini-batches?</p>

    <p>We can more quickly compute new parameters with smaller steps. These smaller steps are mini-batches. They are less accurate than running the full batch, but faster. They are also more likely to be runnable on GPU accelerated systems, where memory is limited.</p>
  </li>
  <li>
    <p>What are the seven steps in SGD for machine learning?</p>

    <ol>
      <li>init: Initialize our weights</li>
      <li>predict: For a given weight, predict the next value</li>
      <li>loss: Compute the loss from our prediction</li>
      <li>gradient: Compute the gradient for how much our loss would change for a small change to a weight</li>
      <li>step: Update our weight</li>
      <li>repeat: return to predict to continue the process</li>
      <li>stop: cease learning</li>
    </ol>
  </li>
  <li>
    <p>How do we initialize the weights in a model?</p>
  </li>
  <li>
    <p>What is “loss”?</p>

    <p>Loss is a computation of how good the model is doing by measuring predictions vs. actuals.</p>
  </li>
  <li>
    <p>Why can’t we always use a high learning rate?</p>

    <p>If the model learns too quickly it will pass or oscillate around the optimal weights. Also, if the learning rate is too high, we may overfit our model.</p>
  </li>
  <li>
    <p>What is a “gradient”?</p>

    <p>A gradient is a computation of how much our loss would change by making a small change to a weight.</p>
  </li>
</ol>

  </div><a class="u-url" href="/education/fastai/2020/09/16/lesson-3.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>Blogging my experiences learning ML via the fast.ai courses.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/tmbeck" title="tmbeck"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
