---
title: "fastai v4: Lesson 3"
description: "Notes from Lesson 3 of fastai v4"
author: "<a href='https://www.linkedin.com/in/tmbeck'>Tim Beck</a>"
toc: true
comments: false
keywords: fastai
categories: [education, fastai]
badges: true
layout: post
---
# Lesson 3

Notes from fastai lesson 3, which is Chapter 4 in the book (the end of Ch. 2 is a "choose your own adventure" where you can continue on to Ch. 3, Data Ethics, or Ch. 4, Under the Hood: Training a Digit Classifier). The video lessons pick up at Ch. 4. But After reading much of Ch. 3, I find the Ethics to be an intriguing topic, especially for leaders of data science organizations, where failure to consider Data Ethics might put your organization (or at least its reputation) at risk.

## Links

* [Lesson 3 Video](https://course.fast.ai/videos/?lesson=3)
* [fastai Forum](https://forums.fast.ai/)
* [A visual intro to numpy and data representation](http://jalammar.github.io/visual-numpy/)

## Notes

### From Chapter 4

Covered images as arrays or tensors of pixels

Cool trick:

Visualizing greyscale images inside of a pandas df by specifying a background graident:

```python
im3_t = tensor(im3)
df = pd.DataFrame(im3_t[4:15,4:22])
df.style.set_properties(**{'font-size':'6pt'}).background_gradient('Greys')
```

#### Stop and Think

>...how a computer might be able to recognize these two digits. What kinds of features might it be able to look at? How might it be able to identify these features? How could it combine them?

This isn't a new topic for me as it overlaps with my *Signals & Systems* coursework, as well as DSP.

* Most straightforward: sum(A-B) = similarity score, identical images == 0
* The above, but after running through edge detection (first derivative)
* Use a kerneling function step-wise compare the outputs of the kernel function between the two systems

Measuring distance in vectors
* L1 norm / mean absolute difference: abs(A-B).mean()
    
    Use `F.l1_loss()`

* L2 norm / root mean square error (RMSE): sqrt(mean((A-B)^2))

    Use `F.mse_loss()`

When computing `mean()` with tensors, `mean()` can take an optional tuple argument that specifies the range of axes over which to compute the mean.

Much of this chapter covers an introduction to numpy and tensors. Vectors and matrices aren't new to me, and even slicing n-dimensional arrays is familiar (I have previous experience in perl and C). But I was struggling with this in python, e.g. the difference between `[:,:]` and `[:][:]`. I found this article by Jay Alammar very helpful: [A visual intro to numpy and data representation](http://jalammar.github.io/visual-numpy/) (thank you Jay!).

Stochastic Gradient Descent (SGD)

So far our primitive model is little more than a math function with no memory and no feedback. To do better we need to be able to train it and to make changes to a set of weights so as to alter the next outcome of the model.

Weights for each pixel. Could use the computed means as weights, where the flattened mean are weights.

Consider the simple function `def pr_eight(x,w) = (x*w).sum()` where `x`, `w` are vectors for the independent variables and the weights, respectively. How would we make this a machine learning "8" classifier?

1. Need to init(w) - seeded rand()?
2. For each image in our set, compute `x*w` e.g. make a prediction.
3. Compute the loss of the model (actual vs. expected)
4. Compute the gradient: a measure of change for each weight that changes the loss
5. Update the weights
6. `goto` 2. until some condition (e.g. the model is good)

Initialize

Start with random. How would random compare against the `mean()` though?

Loss

The convention is that small loss == good, big loss == bad. So we want a function that will measure ourselves as such.

Step

This is a change to the weight that results in a new prediction. Weights can go up or down. Mentions using gradients (calculus). But not clear how to avoid things like local minimums...

Stop

How we know when to stop - e.g. such as when our model begins to get worse at its job.

#### Gradients

* Gradients are a (continuous? since you must take at least the first derivative) function (used the quadratic y=x**2) with a local minimum (since there's a stopping point where y'==0?). It's also symmetric around an axis.
* A random value on the line is chosen (some x) as the initial value
* pytorch provides the `backward()` function to compute the gradient of a layer (array) of a network. This is the *backward pass* through the network. The *forward pass* is where the activation function is calculated.
* This is all handled through object-oriented "magic" in pytorch.
* stepping is done by altering the weights by your learning rate (`lr`) * the gradient of your weights. Recall that `1e-5` is a common rate.
* Takeaway: the gradient is a search for a local minimum. Step size through this search is the learning rate. This is done by backpropagating the affect of a function to a tensor's gradient. In this sense, "all the points" in our array are somewhere on this line and we wish to find a tensor such that the sum of the gradients is closest to zero. Think I said that correctly.

Application:

Let's apply the above to our MNIST imageset.