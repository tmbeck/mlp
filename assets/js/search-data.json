{
  
    
        "post0": {
            "title": "Production from Scratch - fastai Lesson 3 ()",
            "content": "import fastbook fastbook.setup_book() . Retrieve files of our subject from bing image search. The images we search for are downloaded locally. . key = os.environ.get(&#39;AZURE_SEARCH_KEY&#39;, &#39;xyz&#39;) . results = search_images_bing(key, &#39;trilobite&#39;) ims = results.attrgot(&#39;content_url&#39;) len(ims) . 150 . fossil_types = &#39;trilobite&#39;, &#39;crinoid&#39;, &#39;bivalve&#39; path = Path(&#39;critters&#39;) . for o in fossil_types: dest = (path/o) if not dest.exists(): dest.mkdir(exist_ok=True) results = search_images_bing(key, f&#39;{o} fossil&#39;) download_images(dest, urls=results.attrgot(&#39;content_url&#39;)) . Some post processing cleanup might be required to remove empty files, html data, or files that are encoded in a format that PIL cannot understand (such as VP8). They might say they are &#39;.jpg&#39; files, but that does NOT mean that they are! . fastai has a handy get_image_files() function that will recursively identify images and return them as a list and a verify_images() which returns a list of images that are not up to snuff. Then, we use L.map() to unlink (remove) any files in the failed images list. . fns = get_image_files(path) fns . (#437) [Path(&#39;critters/trilobite/00000001.jpg&#39;),Path(&#39;critters/trilobite/00000000.jpg&#39;),Path(&#39;critters/trilobite/00000004.jpg&#39;),Path(&#39;critters/trilobite/00000005.jpg&#39;),Path(&#39;critters/trilobite/00000002.jpg&#39;),Path(&#39;critters/trilobite/00000007.jpg&#39;),Path(&#39;critters/trilobite/00000006.jpg&#39;),Path(&#39;critters/trilobite/00000013.jpg&#39;),Path(&#39;critters/trilobite/00000008.jpg&#39;),Path(&#39;critters/trilobite/00000011.jpg&#39;)...] . failed = verify_images(fns) failed . (#0) [] . failed.map(Path.unlink); . Time to take a look at the images we&#39;ve downloaded. fastai has a handy show_batch() function for its ImageDataLoaders objects so we can get a preview of the images inside jupyter. We must specify item_tfms= or else the widget will not be able to render the wide variety of image resolutions that have been downloaded. . dls = ImageDataLoaders.from_folder(path/&#39;crinoid&#39;, valid_pct=0.2, item_tfms=Resize(256)) dls.valid_ds.items[:3] . [Path(&#39;critters/crinoid/00000087.jpg&#39;), Path(&#39;critters/crinoid/00000065.jpg&#39;), Path(&#39;critters/crinoid/00000045.jpg&#39;)] . dls.show_batch(max_n=40, nrows=4) . dls = ImageDataLoaders.from_folder(path/&#39;trilobite&#39;, valid_pct=0.2, item_tfms=Resize(256)) dls.show_batch(max_n=40, nrows=4) . dls = ImageDataLoaders.from_folder(path/&#39;bivalve&#39;, valid_pct=0.2, item_tfms=Resize(256)) dls.show_batch(max_n=40, nrows=4) . We are now ready to create a DataBlock. The DataBlock will contain our images and labels; it will need to know how to &#39;find&#39; the items, how to separate them into a training set and a validation set, where to get the dependent variable from (the label, e.g. the directory they are in), and last how to transform the image so that we can run CUDA on it. . critters = DataBlock( blocks=(ImageBlock, CategoryBlock), get_items=get_image_files, splitter=RandomSplitter(valid_pct=0.2, seed=42), get_y=parent_label, item_tfms=RandomResizedCrop(256, min_scale=0.3)) . The data block has not yet seen our data, so lets show it the data. . dls = critters.dataloaders(path, bs=64) . dls.valid.show_batch(max_n=10, nrows=2) . dls.train.show_batch(max_n=10, nrows=2) . learn = cnn_learner(dls, resnet34, metrics=error_rate) learn.fine_tune(3) . epoch train_loss valid_loss error_rate time . 0 | 1.940702 | 0.609582 | 0.244186 | 00:09 | . epoch train_loss valid_loss error_rate time . 0 | 0.762174 | 0.398268 | 0.197674 | 00:12 | . 1 | 0.567270 | 0.371190 | 0.151163 | 00:12 | . 2 | 0.462896 | 0.349624 | 0.139535 | 00:12 | . interp = ClassificationInterpretation.from_learner(learn) interp.plot_confusion_matrix() . interp.plot_top_losses(10, nrows=2) . cleaner = ImageClassifierCleaner(learn) cleaner . learn.export() #path.ls(file_exts=&#39;.pkl&#39;) . p = Path() p.ls(file_exts=&#39;.pkl&#39;) . (#1) [Path(&#39;export.pkl&#39;)] . I had some cleanup issues when using the ImageClassifierCleaner() class. After using the widget and then invoking the for loops to unlink and move the images, it threw an exception because filenames already existed. It appears it had problems counting the images/moving the files due to name collisions. It also wasn&#39;t clear what to do after cleaning the dataset - simply running learn.fine_tune(1) did not work since the dataset had changed, so I reinitialized the datablock and retrained the model. This improved performance from an error rate of about 17% to about 14%. .",
            "url": "https://blog.tbeck.io/fossils/jupyter/2020/09/18/Fossils-Production.html",
            "relUrl": "/fossils/jupyter/2020/09/18/Fossils-Production.html",
            "date": " • Sep 18, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "fastai v4: Lesson 3",
            "content": "Lesson 3 . Notes from fastai lesson 3, which is Chapter 4 in the book (the end of Ch. 2 is a “choose your own adventure” where you can continue on to Ch. 3, Data Ethics, or Ch. 4, Under the Hood: Training a Digit Classifier). The video lessons pick up at Ch. 4. But After reading much of Ch. 3, I find the Ethics to be an intriguing topic, especially for leaders of data science organizations, where failure to consider Data Ethics might put your organization (or at least its reputation) at risk. . Links . Lesson 3 Video | fastai Forum | A visual intro to numpy and data representation | . Notes . From Chapter 4 . Covered images as arrays or tensors of pixels . Cool trick: . Visualizing greyscale images inside of a pandas df by specifying a background graident: . im3_t = tensor(im3) df = pd.DataFrame(im3_t[4:15,4:22]) df.style.set_properties(**{&#39;font-size&#39;:&#39;6pt&#39;}).background_gradient(&#39;Greys&#39;) . Stop and Think . …how a computer might be able to recognize these two digits. What kinds of features might it be able to look at? How might it be able to identify these features? How could it combine them? . This isn’t a new topic for me as it overlaps with my Signals &amp; Systems coursework, as well as DSP. . Most straightforward: sum(A-B) = similarity score, identical images == 0 | The above, but after running through edge detection (first derivative) | Use a kerneling function step-wise compare the outputs of the kernel function between the two systems | . Measuring distance in vectors . L1 norm / mean absolute difference: abs(A-B).mean() . Use F.l1_loss() . | L2 norm / root mean square error (RMSE): sqrt(mean((A-B)^2)) . Use F.mse_loss() . | . When computing mean() with tensors, mean() can take an optional tuple argument that specifies the range of axes over which to compute the mean. . Much of this chapter covers an introduction to numpy and tensors. Vectors and matrices aren’t new to me, and even slicing n-dimensional arrays is familiar (I have previous experience in perl and C). But I was struggling with this in python, e.g. the difference between [:,:] and [:][:]. I found this article by Jay Alammar very helpful: A visual intro to numpy and data representation (thank you Jay!). . Stochastic Gradient Descent (SGD) . So far our primitive model is little more than a math function with no memory and no feedback. To do better we need to be able to train it and to make changes to a set of weights so as to alter the next outcome of the model. . Weights for each pixel. Could use the computed means as weights, where the flattened mean are weights. . Consider the simple function def pr_eight(x,w) = (x*w).sum() where x, w are vectors for the independent variables and the weights, respectively. How would we make this a machine learning “8” classifier? . Need to init(w) - seeded rand()? | For each image in our set, compute x*w e.g. make a prediction. | Compute the loss of the model (actual vs. expected) | Compute the gradient: a measure of change for each weight that changes the loss | Update the weights | goto 2. until some condition (e.g. the model is good) | Initialize . Start with random. How would random compare against the mean() though? . Loss . The convention is that small loss == good, big loss == bad. So we want a function that will measure ourselves as such. . Step . This is a change to the weight that results in a new prediction. Weights can go up or down. Mentions using gradients (calculus). But not clear how to avoid things like local minimums… . Stop . How we know when to stop - e.g. such as when our model begins to get worse at its job. . Gradients . Gradients are a (continuous? since you must take at least the first derivative) function (used the quadratic y=x**2) with a local minimum (since there’s a stopping point where y’==0?). It’s also symmetric around an axis. | A random value on the line is chosen (some x) as the initial value | pytorch provides the backward() function to compute the gradient of a layer (array) of a network. This is the backward pass through the network. The forward pass is where the activation function is calculated. | This is all handled through object-oriented “magic” in pytorch. | stepping is done by altering the weights by your learning rate (lr) * the gradient of your weights. Recall that 1e-5 is a common rate. | Takeaway: the gradient is a search for a local minimum. Step size through this search is the learning rate. This is done by backpropagating the affect of a function to a tensor’s gradient. In this sense, “all the points” in our array are somewhere on this line and we wish to find a tensor such that the sum of the gradients is closest to zero. Think I said that correctly. | . Application: . Let’s apply the above to our MNIST imageset. .",
            "url": "https://blog.tbeck.io/education/fastai/2020/09/16/lesson-3.html",
            "relUrl": "/education/fastai/2020/09/16/lesson-3.html",
            "date": " • Sep 16, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "fastai v4: Lesson 2",
            "content": "Lesson 2 . Notes from fastai lesson 2. . Links . Lesson 2 Video | fastai Forum | Model Zoo | ONNX Models | . Notes . Transfer learning: using an existing architecture to create a model trained on a particular data set . Error is one kind of metric, measuring “how well you’re doing” Loss is a measure of performance used to improve the parameters. “Are we learning (adjusting our parameters)?” . Model training . Built a model from Bing API search results (grizzly vs black vs teddy bears) | Used DataBlock() to create a dataset | Used dataloaders to load the data into memory | Used the resnet18 architecture to train a model using the dataset, similar to lesson 1 | Exported the model via pickle to productionize it - model contains the architecture + new parameters + vocabulary (labels) | Used the model to perform inference on images the model has not seen before | Used ImageClassifierCleaner() to clean the dataset | . Next lesson . Deploying to binder, treating your model as if it’s in production by uploading new images to it in “production” | . Jargon . Jargon We Use (again) Description . Label | The data we’re trying to predict (recall the diagram) | . Architecture | A template of the model we are trying to fit. It represents the mathematical function we pass inputs &amp; parameters to. | . Model | An architecture with a specific set of parameters. The parameters may be created through training over one or more epochs. | . Parameters | The values of the model that we can alter trough training. | . Fit or Train | Updating the parameters such that the model is better able to predict our labels. | . Pretrained model | A model with parameters adjusted via training, typically will be fine_tune()d, such as resnet34. | . fine tune | Update a pretrained model for another task, such as making resnet34 recognize cats or dogs. | . epoch | One complete pass through the input data | . metric | A measure of how good the model is to control training via SGD | . validation set | A subset of our data we do not train our model with to measure its performance | . training set | A subset of our data we train our model with that does not include any data from the validation set | . overfitting | Training a model that results in memorization rather than generalization | . cnn | A convolutional neural network, a type of NN suited for computer vision | . Flash Cards: . Architecture vs. Model? A model includes an architecture with a specific set of parameters. These parameters allow the architecture to do something it wasn’t originally designed to do. | . | . Where is Deep Learning good? . Computer Vision - detection &amp; classification | Text - Classifiction &amp; conversation (but not really) | Tabular - Effective on high cardinality datasets, e.g. part numbers and serial numbers | Recommendation Systems (Recsys, aka Collaborative Filtering), but note predictions &lt;&gt; recommendations - because you like to read science fiction, a model might predict you’ll like Aasimov - but that might not be what you want from a recommendation engine e.g. if you’re branching out to Romance. | Multi-modal - Combining the above, capition images, humans in the loop | Various other - NLP, protein | Products in the wild . Arsenal 2, combining a camera with an AI platform | Birdsy, using computer vision to classify birds in real time | Get Writing . This blog :) . Questionaire . Questions 1-12 are annswered in my lesson 1 notes. . What is a p value? . A p-value is the probability of an observed result assuming that the null hypothesis is true. They are terrible and shouldn’t be relied on, per the American Statistical Association. It does not provide a good measure of evidence for scientific conclusions to be made. . See p-value . | What is a prior? . Not clear what this question is asking and it’s not listed in the book. . | Provide an example of where the bear classification model might work poorly in production, due to structural or style differences in the training data. . The model is not well trained on pictures of bears from various angels. The model might struggle with images of bears from above, or from behind, or partial images. . | Where do text models currently have a major deficiency? This is question 1 of Ch 2. in the book. | . | What are possible negative societal implications of text generation models? In situations where a model might make mistakes, and those mistakes could be harmful, what is a good alternative to automating a process? . | What kind of tabular data is deep learning particularly good at? . | What’s a key downside of directly using a deep learning model for recommendation systems? . It doesn’t know how to recommend things outside of its domain that you might still be interested in. . | What are the steps of the Drivetrain Approach? . | How do the steps of the Drivetrain Approach map to a recommendation system? . | Create an image recognition model using data you curate, and deploy it on the web. . | What is DataLoaders? . A class that helps prepare a dataset. . | What four things do we need to tell fastai to create DataLoaders? . How to find the data (get_items) | How to get the dependent and independent variable(s) (get_x, get_y) | Create the data blocks (e.g. images and labels) | How to transform the items, such as Resize(128). | | What does the splitter parameter to DataBlock do? . It defines how your data is split up into a training set and a validation set. . | How do we ensure a random split always gives the same validation set? . Use a seed value by speifying seed=int when calling RandomSplitter(). . |",
            "url": "https://blog.tbeck.io/education/fastai/2020/09/06/lesson-2.html",
            "relUrl": "/education/fastai/2020/09/06/lesson-2.html",
            "date": " • Sep 6, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Fossil Dataset Construction",
            "content": "Introduction . I hacked together most of this code in between completing the first few lessons from the fastai course v3. The final product is a site I host internally so that my local paleontologist can label data for me. :) . This was the first big step in one of my projects. The objective of this notebook was to learn about the reddit API and build a dataset I could use to train a model using the resnet architecture. It also helped me validate my home setup was complete. . What it didn&#39;t teach me is how to integrate with the broader community, which is another reason I wanted to create this blog. . #hide_output import os import sys import re import glob from pathlib import Path import numpy as np import pandas as pd import requests # For generating the widgets import ipywidgets as widgets from ipywidgets import interact, interact_manual from IPython.display import clear_output, display, Image as Img from IPython.core.display import HTML import ipyplot from fastai.vision import * from fastai.vision import core # For interacting with reddit import praw from praw.models import MoreComments # For managing our reddit secrets from dotenv import load_dotenv load_dotenv() . Data Organization . To get started, we will follow the example in the first lessons by using a dataset that is labeled by the directory name. We will store images in the path below, which we will also use for training with fastai. . In this case, images are saved to the path /home/tbeck/src/data/fossils. fastai saves the images using an 8-digit, zero filled identifier. This code does not check for duplicates and does not allow the user to review the existing dataset (e.g. to clean it), although there are some tools now with fastai that might be useful for that purpose. . dataset_path = Path(&#39;/home/tbeck/src/data/fossils&#39;) labels = [x.name for x in dataset_path.iterdir() if (x.is_dir() and x.name not in [&#39;models&#39;])] . PRAW . We must instantiate the reddit PRAW client, so we do so by passing it environment variables loaded from a .env file via the python-dotenv package. Using the API, we can obtain URLs to posted images and scrape the reddit comments (useful for getting hints). . reddit = praw.Reddit(client_id=os.environ[&#39;REDDIT_CLIENT_ID&#39;], client_secret=os.environ[&#39;REDDIT_SECRET&#39;], password=os.environ[&#39;REDDIT_PASSWORD&#39;], user_agent=os.environ[&#39;REDDIT_USER_AGENT&#39;], username=os.environ[&#39;REDDIT_USERNAME&#39;]) fossilid = reddit.subreddit(&#39;fossilid&#39;) . We need some helper functions to retrieve the images and save them to our dataset. I found that URLs obtained from reddit need some post processing, otherwise they do not render properly. . def download_image(url, dest=None): &quot;&quot;&quot;Given a URL, saves the image in a format the fastai likes.&quot;&quot;&quot; dest = Path(dest) dest.mkdir(exist_ok=True) files = glob.glob(os.path.join(dest, &#39;*.jpg&#39;)) + glob.glob(os.path.join(dest, &#39;*.png&#39;)) i = len(files) suffix = re.findall(r&#39; . w+?(?=(?: ?|$))&#39;, url) suffix = suffix[0] if len(suffix)&gt;0 else &#39;.jpg&#39; try: core.download_url(url, dest/f&quot;{i:08d}{suffix}&quot;, overwrite=True, show_progress=False) except Exception as e: f&quot;Couldn&#39;t download {url}.&quot; def get_image(url, verbose=False): &quot;&quot;&quot;Given a URL, returns the URL if it looks like it&#39;s a URL to an image&quot;&quot;&quot; IMG_TEST = &quot; .(jpg|png)&quot; p = re.compile(IMG_TEST, re.IGNORECASE) if p.search(url): if verbose: print(&quot;url to image&quot;) return url IMGUR_LINK_TEST = r&quot;((http|https)://imgur.com/[a-z0-9]+)$&quot; p = re.compile(IMGUR_LINK_TEST, re.IGNORECASE) if p.search(url): if verbose: print(&quot;imgur without extension&quot;) return url + &#39;.jpg&#39; IMGUR_REGEX_TEST = r&quot;((http|https)://i.imgur.com/[a-z0-9 .]+?(jpg|png))&quot; p = re.compile(IMGUR_REGEX_TEST, re.IGNORECASE) if p.search(url): if verbose: print(&quot;imgur with extension&quot;) return url return None class Error(Exception): def __init__(self, msg): self.msg = msg class SubmissionStickiedError(Error): pass class SubmissionIsVideoError(Error): pass class SubmissionNotAnImageError(Error): pass class DisplayError(Error): pass . Now we can query reddit for the images. The method below to build the dataset is a little clunky (I create arrays for each column of data and take great steps to be sure they are equal length). A better way would be to delegate creating this data structure to a single function so that the code below is less complex. . # Fetch submissions for analysis and initialize parallel arrays submissions = [] images = [] top_comments = [] errors = [] verbose = False for i, submission in enumerate(reddit.subreddit(&quot;fossilid&quot;).new(limit=None)): submissions.append(submission) images.append(None) top_comments.append(None) errors.append(None) try: if submission.stickied: raise SubmissionStickiedError(&quot;Post is stickied&quot;) if submission.is_video: raise SubmissionIsVideoError(&quot;Post is a video&quot;) if get_image(submission.url): if verbose: print(f&quot;Title: {submission.title}&quot;) images[i] = get_image(submission.url) try: if verbose: display(Img(get_image(submission.url), retina=False, height=400, width=400)) except Exception as err: if verbose: print(f&quot;Failed to retrieve transformed image url {get_image(submission.url)} from submission url {submission.url}&quot;) raise DisplayError(f&quot;Failed to retrieve transformed image url {get_image(submission.url)} from submission url {submission.url}&quot;) submission.comments.replace_more(limit=None) for top_level_comment in submission.comments: if verbose: print(f&quot;Comment: t{top_level_comment.body}&quot;) top_comments[i] = top_level_comment.body break else: raise SubmissionNotAnImageError(&quot;Post is not a recognized image url&quot;) except Exception as err: submissions[i] = None images[i] = None top_comments[i] = None errors[i] = err.msg df = pd.DataFrame({&#39;submissions&#39;: submissions, &#39;images&#39;: images, &#39;comments&#39;: top_comments, &#39;errors&#39;: errors}) df.dropna(how=&#39;all&#39;, inplace=True) df.dropna(subset=[&#39;images&#39;], inplace=True) . Widget Requirements . Be able quickly jump around in the dataset | Render the image | Render a hint from the top reddit comment | Save the image to disk | Skip this image and show the next one | Reset the form | debug = False output2 = widgets.Output() reset_button = widgets.Button(description=&#39;Reset&#39;) def on_reset_button_click(_): with output2: clear_output() int_range.value = 0 classes_dropdown.value = None new_class.value = &#39;&#39; reset_button.on_click(on_reset_button_click) save_button = widgets.Button( description=&#39;Save&#39;, disabled=False, button_style=&#39;&#39;, # &#39;success&#39;, &#39;info&#39;, &#39;warning&#39;, &#39;danger&#39; or &#39;&#39; tooltip=&#39;Save&#39;, icon=&#39;check&#39; ) skip_button = widgets.Button( description=&#39;Skip&#39;, disabled=False, button_style=&#39;&#39;, # &#39;success&#39;, &#39;info&#39;, &#39;warning&#39;, &#39;danger&#39; or &#39;&#39; tooltip=&#39;Skip&#39;, icon=&#39;&#39; ) int_range = widgets.IntSlider( value=0, min=0, max=len(df) - 1, step=1, description=&#39;Submission:&#39;, disabled=False, continuous_update=False, orientation=&#39;horizontal&#39;, readout=True, readout_format=&#39;d&#39; ) img = widgets.Image( value=requests.get(df.iloc[int_range.value][&#39;images&#39;]).content, format=&#39;png&#39;, width=480, height=640, ) reddit_link = widgets.Label(&#39;Link: &#39; + str(df.iloc[int_range.value][&#39;submissions&#39;].url)) comment = widgets.Label(&#39;Hint: &#39; + str(df.iloc[int_range.value][&#39;comments&#39;])) local_options = [x.name for x in dataset_path.iterdir() if (x.is_dir() and x.name not in [&#39;models&#39;])] local_options.sort() classes_dropdown = widgets.Dropdown( options=[None] + local_options, value=None, description=&#39;Class:&#39;, disabled=False, ) # Free form text widget new_class = widgets.Text( value=None, placeholder=&#39;&#39;, description=&#39;New Class:&#39;, disabled=False ) def on_save_button_click(_): err=None if len(new_class.value) &gt; 0: label = new_class.value elif classes_dropdown.value: label = classes_dropdown.value else: err = &quot;You must specify a label to save to.&quot; with output2: clear_output() if err: print(err) else: if debug: print(f&quot;Would fetch index {int_range.value} from {df.iloc[int_range.value][&#39;images&#39;]} to {dataset_path/label}&quot;) else: Path(Path(dataset_path)/Path(label)).mkdir(exist_ok=True) core.download_url(f&quot;{df.iloc[int_range.value][&#39;images&#39;]}&quot;, f&quot;{dataset_path/label}&quot;, show_progress=False, timeout=10) local_options = [x.name for x in dataset_path.iterdir() if (x.is_dir() and x.name not in [&#39;models&#39;])] local_options.sort() classes_dropdown.options = [None] + local_options int_range.value = int_range.value + 1 classes_dropdown.value = None new_class.value = &#39;&#39; def on_skip_button_click(_): with output2: clear_output() int_range.value = int_range.value + 1 classes_dropdown.value = None new_class.value = &#39;&#39; save_button.on_click(on_save_button_click) skip_button.on_click(on_skip_button_click) def on_value_change(change): img.value = requests.get(df.iloc[change[&#39;new&#39;]][&#39;images&#39;]).content reddit_link.value = &#39;Link: &#39; + str(df.iloc[int_range.value][&#39;submissions&#39;].url) comment.value=&#39;Hint: &#39; + str(df.iloc[change[&#39;new&#39;]][&#39;comments&#39;]) #with output2: # print(change[&#39;new&#39;]) int_range.observe(on_value_change, names=&#39;value&#39;) buttons = widgets.HBox([save_button, skip_button, reset_button]) entry = widgets.HBox([classes_dropdown, new_class]) things = widgets.VBox([int_range, img, reddit_link, comment, entry, buttons, output2]) display(things) . Widgets . To build the dataset and gain context, I created a custom widget for my local paleontologist to use. Here, she can easily navigate the images and apply labels. This widget was a composite of multiple ipywidgets to appear as a single form: . A slider so they can quickly jump between submissions | Drop down and text fields that can be populated with fixed or freeform data | Buttons for saving data, advancing, and resetting the form. | . In addition, I show the reddit comment using a label as a hint to the user. . When the widget is rendered, it uses the DataFrame to retrieve the image from the url. . The &#39;Class&#39; dropdown is created from the labels loaded above. When &#39;New Class&#39; is not empty and &#39;Save&#39; is pressed, a new directory is created (if needed) and the image is saved there using fastai (see download_image()). . Here is what the widget ends up looking like: . Reviewing The Dataset . Because the data is stored in a DataFrame, we can easily manipulate the information we&#39;ve scraped. The ipyplot package is useful for generating thumbnails from a series of urls. This makes it easy to quickly review what&#39;s been scraped. . ipyplot.plot_images(df[&#39;images&#39;], max_images=80, img_width=100) . Lessons Learned . Functionality here is somewhat similar to what other widgets can do, such as superintendent. But this was a good exercise to dust off my ipywidget skills, experiment with interacting with reddit&#39;s API, and building a custom dataset using fastai. . I built this notebook when fastai&#39;s course v3 was still out, when Google images was being scraped rather than the Bing search API being used. .",
            "url": "https://blog.tbeck.io/projects/jupyter/2020/09/06/fossil-dataset.html",
            "relUrl": "/projects/jupyter/2020/09/06/fossil-dataset.html",
            "date": " • Sep 6, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "The story so far",
            "content": "The Story so Far . Ok, so this all started around July 2020 when I decided to invest in my own education and learn more about what some of my team members were working on. One of them suggested fast.ai so I decided to give it a try. . At the time, the course v3 was in “production”. Since then, v4 has been released. . Setup . Being the hands on engineer that I am, I decided to build my own system for machine learning from spare parts around the house. I already have experience deploying jupyter for use by development teams and I am no stranger to Linux. The fast.ai Getting Started guide originally suggested using managed services such as Google Cloud or AWS, but it was straightforward enough to get something working that I’ve documented it below. . Note that this isn’t recommended by Jeremy in the video lesson, but I decided to do it to understand what is going on behind the scenes (and why pay AWS when I have the hardware and free solar energy). . Hardware . These are parts I had laying around the house. The key if you want GPU acceleration with pytorch is to have the right GPU; too old and your GPU won’t support the necessary GPU compute capabilities. Below is the harware I had on hand: . Intel i7-4770 | 32 GB of RAM | NVIDIA Corporation GM204 [GeForce GTX 970] (rev a1); GPU compute capability 5.2 | 1 TB SSD (actually an upgrade; spinning rust was unbearably slow due to the low number of random IOPS) | . Software . Linux is my preferred operating system in general for hacking, so I went with the recently released Ubuntu 20.04 LTS Server. . I’ve been using anaconda as my python distribution of choice for many years now, so I grabbed the python 3.8 x86_64 for Linux package. . I also grabbed docker.io. Nvidia has a solution for doing GPU compute that requires docker to be installed, as well. . Since fastai uses pytorch, and pytorch only support CPU or GPU via cudatoolkit, I needed the nvidia.ko kernel module, the necessary nvidia cuda toolkit libraries, and the right packages in a conda environment. . I installed nvidia-dkms-450 to provide nvidia drivers for my GTX 970. . Nvidia provides a CUDA toolkit 10.2 Ubuntu repo. Even though it says 18.04, it worked fine for me on 20.04. Just follow the instructions to install the cuda package. . You might also find the NVIDIA CUDA Installation Guide for Linux helpful for troubleshooting installation issues. . Finally, to take advantage of the GPU you must install the GPU-accelerated version of pytorch. Only the conda instructions are needed since the system python isn’t used. . $ conda install pytorch torchvision cudatoolkit=10.2 -c pytorch . Reboot as needed. . Integration &amp; Test . Finally we can create a new conda environment, install the necessary packages, and verify pytorch can see and use the GPU. . Create a fastai conda environment with python 3.8 . $ conda create -n fastai python=3.8 $ conda activate fastai . | Install fastai in conda, as per their Install Instructions. It might take a while for conda to determine which channel to get the packages from. . $ conda install -c fastai -c pytorch -c anaconda fastai gh anaconda $ conda install -c fastai -c pytorch -c anaconda fastai gh anaconda cudatoolkit=10.2 Collecting package metadata (current_repodata.json): done Solving environment: done ## Package Plan ## environment location: /home/tbeck/anaconda3 added / updated specs: - anaconda - cudatoolkit=10.2 - fastai - gh The following packages will be downloaded: package | build |-- ca-certificates-2020.7.22 | 0 132 KB anaconda certifi-2020.6.20 | py37_0 159 KB anaconda conda-4.8.4 | py37_0 3.0 MB anaconda cudatoolkit-10.2.89 | hfd86e86_1 540.0 MB anaconda gh-0.11.1 | 0 5.5 MB fastai openssl-1.1.1g | h7b6447c_0 3.8 MB anaconda Total: 552.5 MB The following NEW packages will be INSTALLED: gh fastai/linux-64::gh-0.11.1-0 The following packages will be SUPERSEDED by a higher-priority channel: ca-certificates pkgs/main --&gt; anaconda certifi pkgs/main --&gt; anaconda conda pkgs/main --&gt; anaconda cudatoolkit pkgs/main --&gt; anaconda openssl pkgs/main --&gt; anaconda Proceed ([y]/n)? . | Once installed, you can quickly test that your GPU is seen and used from the command line like so: . $ python -c &#39;import torch; print(torch.cuda.get_device_name())&#39; GeForce GTX 970 $ python -c &#39;import torch; print(torch.rand(2,3).cuda())&#39; tensor([[0.3352, 0.0835, 0.5349], [0.3712, 0.2851, 0.8767]], device=&#39;cuda:0&#39;) . | If you see your expected video card and a tensor returned, you’re all set. If you have multiple GPU’s installed, you may need to specify which one to use. Check out this stack overflow article on how to set the appropriate environment variables for the command line and for jupyter to work. . A note on updates . I’ve noticed that if you update conda using conda update --all, it will try to pull in the latest version of cudatoolkit, which as of this writing is cudatoolkit-11.0.221-h6bb024c_0. This is safe to do, but you will need to downgrade back to cudatoolkit-10.2. This seems to be due to how anaconda handles/prioritizes packages from various channels. Below is an example. . Upgrading conda (only showing cudatoolkit for visbility - your output will differ) . $ conda update --all ... The following packages will be UPDATED: cudatoolkit anaconda::cudatoolkit-10.2.89-hfd86e8~ --&gt; pkgs/main::cudatoolkit-11.0.221-h6bb024c_0 . Downgrading cudatoolkit . $ conda install pytorch torchvision cudatoolkit=10.2 -c pytorch ... added / updated specs: - cudatoolkit=10.2 The following packages will be downloaded: package | build |-- cudatoolkit-10.2.89 | hfd86e86_1 365.1 MB Total: 365.1 MB The following packages will be DOWNGRADED: cudatoolkit 11.0.221-h6bb024c_0 --&gt; 10.2.89-hfd86e86_1 . Troubleshooting . If you get the below trying to use torch then cuda isn’t working as expected. . $ python -c &#39;import torch; print(torch.rand(2,3).cuda())&#39; Traceback (most recent call last): File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt; File &quot;/home/tbeck/anaconda3/envs/fastai/lib/python3.7/site-packages/torch/cuda/__init__.py&quot;, line 192, in _lazy_init _check_driver() File &quot;/home/tbeck/anaconda3/envs/fastai/lib/python3.7/site-packages/torch/cuda/__init__.py&quot;, line 95, in _check_driver raise AssertionError(&quot;Torch not compiled with CUDA enabled&quot;) AssertionError: Torch not compiled with CUDA enabled . Note that trying to install cudatoolkit=10.2 alone might result in this error, so be sure that pytorch and torchvision are included when specifying cudatoolkit=10.2. . Preparing for class . There are two sets of notebooks for the class: . The fastbook, a guided set of notebooks with prose for following along in the videos: fastbook | The same notebooks as a study aid: course-v4 | . Please consider showing your support by buying the fastbook: Deep Learning for Coders with fastai and PyTorch: AI Applications Without a PhD . I setup my own area for hacking: . $ mkdir ~/src $ cd ~/src &amp;&amp; git clone https://github.com/fastai/fastbook.git &amp;&amp; git clone https://github.com/fastai/course-v4.git . Now that CUDA is working and we have the code, I prefer fire up my jupyter notebook in a screen session. To do this I generated a https://jupyter-notebook.readthedocs.io/en/stable/config.html via jupyte notebook --generate-config and wrote it to ~/.jupyter/jupyte_notebook_config.py. Then I made it listen on 0.0.0.0 so I can reach it from my LAN (or anywhere in the world via wireguard!). . Now I just run screen, activate conda with conda activate fastai, and finally start jupyter with jupyter notebook. For tricks on using screen see this quickreference . If you prefer to use jupyter lab, you’ll need to conda install jupyterlab and run jupyter lab instead. .",
            "url": "https://blog.tbeck.io/linux/education/fastai/2020/09/05/the-story-so-far.html",
            "relUrl": "/linux/education/fastai/2020/09/05/the-story-so-far.html",
            "date": " • Sep 5, 2020"
        }
        
    
  
    
  
    
        ,"post6": {
            "title": "fastai v4: Lesson 1",
            "content": "Lesson 1 . Notes from fastai lesson 1. . Links . Lesson 1 Video | fastai Forum | . Notes . Discussed what’s needed to take this course (not much!). | Discussed history of AI and how deep neural networks came to be. Mostly news to me, 1/10. | Introduced Jupyter, ipywidgets, REPL. Mostly seen this before, 7/10. | Introduced ML (repeat, but still new, 4/10) take an input, process it, get an output. | Samuel’s terminology: take inputs &amp; weights into a model, generate results | Add feedback: measure performance of the results (a metric), then change weights. Rinse &amp; repeat. | Different weights to the model allows it to do a different task | Universal approximation theorem: theory that a neural network could solve any problem to any level of accuracy. | Need a way to update weights - SGD: stochastic gradient descent - to update the weights | . | ML Limitations . A model can only be created from data | A model can only learn from patterns in the inputs | A model can only make a prediction - actions happen externally | . Labeled data is key and often missing - good part, bad part, etc. . It is important to note that a feedback loop can be created, resulting in a causal relationship where none existed before. Jeremy gave an example in lesson 1. This goes to ethics along ML and understanding inherent biases in your training dataset that you may not be aware of. . | The fastai notebook . Intro to fastai. Discuss REPL, data sets, etc. . from fastai.vision.all import * . learner = cnn_learner(dls, resnet34, metrics=error_rate) . Here cnn_learner is a function that generates our model, dls is our dataset, resnet34 is our architecture, and error_rate is a loss function for feedback. . resnet34 is a predefined neural network trained on images that is free to use. 34 indicates it has 34 layers, more layers requires more GPU memory. . error_rate is computed on data not used in training, also known as a holdout set or validation set, to help avoid overfitting. Might need to increase the size of the holdout set to avoid overfitting. The ambiguiuty is actually important in model building, otherwise the model will only know how to recognize images it has seen before. . | Other uses . segmentation: figuring out what every pixel in an image is (what label it corresponds to) . For a PWA, labels might be . trace, screw, via, component | . A larger training set might allow . trace, via, capacitor, resistor, inductor, transistor, integrated circuit, etc. | . Project idea: create an architecture for use in image segmentation that is trained on images of various PWAs (raspberry Pis, motherboards, etc.) to recognize features. . tabular data: fitting a model to predict salary based on a variety of parameters, predicting ratings a user might give a movie they haven’t seen based on previous ratings they have given (known as collaboration, used in recommendation engines) . | Jargon . Keyword Description . Architecture | The “program” we are running. Often synonymous with model, but represents its functional form.. | . Parameters | The “weights” into the “program” that alter its performance | . Predictions | The output of our architecture, computed from independent variables which does not include labels | . Labels | The targets or dependent variables, assumed to be true for a given prediction. | . Loss | A metric of performance we measure our model by: how well did our prediction (computed from independent variables) match our labels (our dependent variables)? | . Model | The combination of the parameters and architecture that can act on inputs to generate a prediction. | . Inputs | The data on which the model acts to generate a prediction. No inputs (data) == No predictions! | . Action | The decision made from reasoning about a given prediction. | . Transfer learning | Using a pre-trained model for a task other than what it was originally intended for (via training) | . Fine Tuning | A transfer learning technique that updates the parameters of a pretrained model by training for additional epochs using a different task from that used for pretraining. This is also known as fitting. | . head | The last and newly added layer of a model which trains it on a particular data set. This replaces the previous layer when cnn_learner is used. | . epoch | One complete training of the model on the complete dataset | . Over fitting | When a model learns the unique characteristics of a dataset rather than a generalization, limiting its ability to do inference on new data. | . Howard, Jeremy. Deep Learning for Coders with fastai and PyTorch . O’Reilly Media. Kindle Edition. . From the loss, we can /update/ the weight for a given input, thus allowing our system to learn. . Homework . Do the questionaire, run the notebooks, etc. . Questionnaire . Some of these questions were answered after watching both videos and reading chapter 1 of the book. Although this blog post is titled lesson 1, chapter 1 is covered in the first two lessons. Since I’m consuming the material one lesson at a time I’ll continue to follow this pattern. . (If you’re not sure of the answer to a question, try watching the next lesson and then coming back to this one, or read the first chapter of the book.) | . Do you need these for deep learning? . Lots of math T / F . False: Much of the math is taken for you thanks to libraries such as numpy. | . Lots of data T / F . False: Relatively small data sets can train world class models - no need for “Big Data”! Lots of expensive computers T / F . | False: You can use consumer grade hardware if you have it or take advantage of Google Cloud, AWS, Colab, etc. . | . A PhD T / F . False: You will need some knowledge of programming and understanding of your data set, but a PhD is not needed. | . | Name five areas where deep learning is now the best in the world. . NLP (natural language processing) | Computer Vision | Medicine | Biology | Image generalization | Forecasting | Robotics (grip strength, movement) | | What was the name of the first device that was based on the principle of the artificial neuron? . | The Mark I Perceptron (sounds like something from Fallout!) . Based on the book of the same name, what are the requirements for parallel distributed processing (PDP)? . A set of processing units | A state of activation | An output function for each unit | A pattern of connectivity between units | A propagation rule for patterns to pass between units via a pattern of connectivity | An activation rule that controls the output for a given input and current state of a unit | A learning rule that modifies patterns of connectivity | An environment to operate in (often overlooked) | . | What were the two theoretical misunderstandings that held back the field of neural networks? . That because a single layer of neurons could not simulate an XOR, neural networks were a dead end | DNN would be too big and too slow to be useful | . | What is a GPU? . A Graphcis Processing Unit, designed for massively parallel execution of floating point vectors. | . | Open a notebook and execute a cell containing: 1+1. What happens? . It computes and displays the output (2) | . | Follow through each cell of the stripped version of the notebook for this chapter. Before executing each cell, guess what will happen. . Done in the notebook… | . | Complete the Jupyter Notebook online appendix. . Done | . | Why is it hard to use a traditional computer program to recognize images in a photo? . Requires a pixel by pixel search of the image, or something akin to a kernel to review pixel groupings, with significant comparison and cyclic complexity. Here, we tell the computer what to think, when to think it - we are explicit. This is converse to machine learning, which learns the patterns in the data and uses those patterns to recognize images in a photo. | . | What did Samuel mean by “weight assignment”? . Weight assignments are variables used in a model to alter the performance of the program (model). The weight assignments must have an automatic means to be updated, so as to provide feedback to improve the performance of the model for its respective purpose. | . | What term do we normally use in deep learning for what Samuel called “weights”? . We call them “parameters”. | . | Draw a picture that summarizes Samuel’s view of a machine learning model. . | * results = model(inputs, weights) * performance = results - actuals * weights = performance(weights) Essentially, the inputs of a model are used to generate results. Those results have a certain performance, which is improved by modifying the weights and repeating the operation. . Why is it hard to understand why a deep learning model makes a particular prediction? . Because of the complexity of the “black box” - it would be necessary to instrument each layer of a neural network in order to understand why. In other words, the why is the entirety of the state machine, although some parts of the state machine may be better than others. | . | What is the name of the theorem that shows that a neural network can solve any mathematical problem to any level of accuracy? . Universal approximation theorem (reminds me of using taylor series expansion to approximate various mathematical functions) | . | What do you need in order to train a model? . You need a dataset and an architecture (a pretrained model) | . | How could a feedback loop impact the rollout of a predictive policing model? . Inherent bias in the dataset could lead to increased policing of a particular population representative of the bias, resulting in additional policing, enhancing the existing dataset bias. | . | Do we always have to use 224×224-pixel images with the cat recognition model? . No, but larger images or sizes may impact performance of the GPU (meaning it will take longer for the model to learn) | . | What is the difference between classification and regression? . Classification predicts the category of a given input (think predicting finite enumerations) | Regression predicts a future numeric value, like the temperature tomorrow | . | What is a validation set? What is a test set? Why do we need them? . A validation or hold out set is a subset of a dataset that we do not train the model with, but we use to grade the model’s performance. | A test set is a dataset we do not show even ourselves, to avoid introducing bias through EDA and model training. | We need these to avoid overfitting or introducing bias into our models | . | What will fastai do if you don’t provide a validation set? . It will automatically use a subset of your data as the validation set. | . | Can we always use a random sample for a validation set? Why or why not? . If we used a random sample for validation, then repeated training (e.g. resampling the random data set and training the model on the resampled data) we could overfit our model. | If we use a static sample for validation, then repeated training won’t be able to overfit, because it will never learn from the validation set. | For example, with time series data we may hold out a specific period (say the last two weeks) of data as a validation set, rather than a random sampling of the dataset.. | . | What is overfitting? Provide an example. . Overfitting is the process of a model “memorizing” a given dataset. This can happen when our learning rate continues to increase and our error rate tends to zero. For example, if you overtrained a model on your entire dataset of cats, it would not be able to perform inference on new cat photos it has never seen before. | . | What is a metric? How does it differ from “loss”? . A metric is all we care about: it is how well our model performs on our validation set. | . | How can pretrained models help? . Pretrained models can be used to train a new model (transfer learning) | . | What is the “head” of a model? . The head of the model is the layer who’s parameters are modified by training | . | What kinds of features do the early layers of a CNN find? How about the later layers? . This depends, but a CNN might first learn things like basic shapes or color gradients. Later layers might recognize repeating patterns or more specific shapes. | . | Are image models only useful for photos? . No! It’s possible to create images from many different types of data, including log files, executables, sound, etc. This can be done by transforming data to grayscale, or using existing visualization techniques (such as FFT). | . | What is an “architecture”? . An architecture is a pretrained model from a specific dataset. resnet is an example of an architecture used in this book for computer vision. | . | What is segmentation? . segmentation is the label of individual pixels of data in an image, e.g. applying the label “car” to the parts of an image with a car in it. | . | What is y_range used for? When do we need it? . y_range | . | What are “hyperparameters”? . A hyperparameter is a parameter about a parameter. When choosing new hyper parameters we must be careful or we may inadvertently introduce bias or overfitting the validation data, basically “finding the answer we want to hear”. | . | What’s the best way to avoid failures when using AI in an organization? . To understand why a validation and test data set are needed and to ensure they are properly used when developing a model. | . | Further Research . I skipped these because I thought the answers were easy, but decided to return to them for completeness. . Why is a GPU useful for deep learning? How is a CPU different, and why is it less effective for deep learning? . GPU’s contain thousands of massively parallel execution cores optimized for floating point arithmetic and matrix math. They contain relatively small amounts of extremely high bandwidth memory. These properties make them advantageous for DL, but inhibit them from being used for, say, running a Web Browser. CPU’s are general purpose computing devices, containing substantially fewer cores, slower memory speed, and tend to favor integer math over floating point math or matrix math (although Intel has tried to change this with AVX512…poorly). A key performance indicator for both CPU and GPUs that tend to lead to effective DL is the FLOP (floating point operations). GPUs typically have substantially higher FLOPs than CPUs. | . | Try to think of three areas where feedback loops might impact the use of machine learning. See if you can find documented examples of that happening in practice. . Elections: if a model with a feedback loop is used to predict election outcomes and has substantial influence on a population of voters, it could dissentivize voters from turning out for an election. While this might be most visible for American presedential races, it could affect less publicized elections such as those for ballot measures and Congress. I don’t think Five-Thirty-Eight is this…yet. . | With the recent Facebook news and Russian influence in American Democracy, I think we’re beginning to see a trend of adversarial machine learning, which is exploiting a competitors production models to destablize or influence its outcome. I think nation states and other actors will continue to use feedback loops in active learning systems. . | Search results. I Google’d a whiskey bottle to read a review before purchasing it. Now Google shows me whiskey reviews in my news feed, displacing other news articles. Reading these seems to reinforce the model and increases the number of articles I see. Since the number of articles I can read is limited, there is no way to “tone down” the model without telling it I’m simply not interested in whiskey. It’s kind of interesting because in general you implicitly train Google. You can’t implicitly untrain it (e.g. there is no decay function or “forgetting”, or it operates on a time horizon I havent seen yet). . | |",
            "url": "https://blog.tbeck.io/education/fastai/2020/09/05/lesson-1.html",
            "relUrl": "/education/fastai/2020/09/05/lesson-1.html",
            "date": " • Sep 5, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": ". Timothy Beck I recieved a degree in Electrical Engineering from UCSD in 2007. I’ve worked most of my engineering career at Viasat, where I’ve enjoyed career mobility by being a test engineer, software engineer, and hardware engineer, later becoming a cross-functional leader of an engineering organization comprised of mechanical, hardware (including programmable logic), and software engineers. Much of my careeer has been focused on constructing what would now be considered “well-labeled datasets”, as well as web-based analysis tools for computing process capability metrics, supporting root cause analysis, providing insight into product performance, general process automation, etc. . . Since learning more about data science and machine learning, I’ve begun exploring the intersection of ML and manufacturing, particularly how ML can accelerate manufacturing &amp; test processes. I feel passionately that this technology can be used to improve product design by providing greater context and understanding by combining manufacturing, test, and operational datasets. . I first started diving into machine learning myself using the fast.ai v3 course, but switched to v4 of the course when it was recently released. In Lesson 3 of v4 of the course, Jeremy recommends to write about what you are learning. Once I saw fastpages1, I figured it was time to start writing. They had me once I found out I could convert jupyter notebooks to blog posts! . I created this site to help me document my journey, study, challenge myself, and show off what I’ve learned. . This website is powered by fastpages1. . Images are &copy; Tim Beck unless noted otherwise, all rights reserved. a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; &#8617;2 . |",
          "url": "https://blog.tbeck.io/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
      ,"page3": {
          "title": "Useful Links",
          "content": "Links for leaders . Care &amp; Feeing of Data Scientists Suggest reading this every few months to be sure you are “on course” if you are new to leading DS | . | . Links for Learning ML . fast.ai Courses v4 course-v4 | fastbook | . | Machine Learning from Scratch | . Other useful links . Screen Quick Reference | Markdown Cheatsheet | Test Markdown Post | . Datasets . fastai Datasets | . Data Science Links . Text Processing . Calculating String Similarity in Python Useful for quantizing free form text entry fields for comparison purposes | . | .",
          "url": "https://blog.tbeck.io/links/",
          "relUrl": "/links/",
          "date": ""
      }
      
  

  
      ,"page4": {
          "title": "Projects",
          "content": "Where failure isn’t just an option, it’s expected. . Projects in progress . Fossil Identification . Reddit is a popular social media website with user submitted content. Subreddit r/fossils frequently has posts such as this one, asking for identification (there is another subreddit, r/fossilid, for this purpose). Could we train a model to classify such images? . Hypothesis: A machine learning model could ultimately be used by the scientific community to identify species, provided a sufficiently large dataset. . Objective: Train a model that can recognize fossils. Create a reddit bot that can reply to posts with classifications, allowing for replies on various subreddits that can help train the model. . Background: For obvious reasons, idenification and taxonomy are important topics for paleontologists. Here, I will train a model using labels that provide as much or more detail as class, as per modern taxonomic classification, with the ultimate goal that a model can identify a specific species (or conversely, identify a species it does not recognise). . See Trilobite evolutionary rates constrain the duration of the Cambrian explosion for how trilobites are classified. A model trained on sufficiently large data set should be able to identify an entire class (e.g. it can identify a fossil’s species if that fossil is known in class trilobita). . Methodology: To do this, I use the same subreddits as a source for material in order to build a dataset. The model will use resnet34 as its architecture. I build upon my growing knowledge from the fastai courses to achieve this. . Exploration . Based on the experience creating a model that can identify teddies (fastai course v3), then cats vs. dogs (course v4), this is plausible but we will need to build a dataset. . Began by investigating solutions for scraping images off reddit. . Initial solution: use ParseHub to scrape reddit and download images. Pros Low code, visually build scraping logic | Output can be automated and available in CSV via API | . | Cons Eventually, not free | Results inconsistent, particularly with the “new” reddit site layout | Slow (unless you pay for it) | Costs $$$ | . | . Tried this solution. Limited success, thanks to a number of tutorials out there showing how to scrape old.reddit.com. This is a no go. . | Better solution: use praw, tutorial here and here on towardsdatascience. Obtain API creds, write own scraping logic. Build into a notebook. | Ideas for new projects . Coming soon .",
          "url": "https://blog.tbeck.io/projects/",
          "relUrl": "/projects/",
          "date": ""
      }
      
  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page12": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://blog.tbeck.io/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}